---
---

@article{almudevar2022unsupervised,
  bibtex_show={true},
  title={Unsupervised anomaly detection applied to $\Phi$-OTDR},
  author={Almudévar, Antonio and Sevillano, Pascual and Vicente, Luis and Preciado-Garbayo, Javier and Ortega, Alfonso},
  journal={Sensors},
  volume={22},
  number={17},
  pages={6515},
  year={2022},
  publisher={MDPI},
  abstract={Distributed acoustic sensors (DASs) based on direct-detection Φ-OTDR use the light–matter interaction between light pulses and optical fiber to detect mechanical events in the fiber environment. The signals received in Φ-OTDR come from the coherent interference of the portion of the fiber illuminated by the light pulse. Its high sensitivity to minute phase changes in the fiber results in a severe reduction in the signal to noise ratio in the intensity trace that demands processing techniques be able to isolate events. For this purpose, this paper proposes a method based on Unsupervised Anomaly Detection techniques which make use of concepts from the field of deep learning and allow the removal of much of the noise from the Φ-OTDR signals. The fact that this method is unsupervised means that no human-labeled data are needed for training and only event-free data are used for this purpose. Moreover, this method has been implemented and its performance has been tested with real data showing promising results.},
  pdf={https://www.mdpi.com/1424-8220/22/17/6515/pdf},
  preview={sensors-22-06515-g006.png},
}

@inproceedings{AlmudevarUZ2022,
  bibtex_show={true},
  author={Almudévar, Antonio and Ortega, Alfonso and Vicente, Luis and Miguel, Antonio and Lleida, Eduardo},
  title={Vision Transformer based Embeddings Extractor for Unsupervised Anomalous Sound Detection under Domain Generalization},
  year=2022,
  booktitle={DCASE Challenge},
  abstract = "Anomalous sound detection (ASD) is the task of identifying if a sound is normal or anomalous with respect to a given reference. In most scenarios, we have a large amount of normal data to design our model, but little or no anomalous data. When this situation occurs, the problem can be approached in an unsupervised manner, i.e., only normal data is used for design. In this report we present a solution for the DCASE2022 task 2 (Unsupervised Anomalous Sound Detection for Machine Condition Monitoring Applying Domain Generalization Techniques), which aims to address the ASD problem under domain generalization. This means that the data to develop the system belongs to the source domain, while the test data can belong to this domain or to a different one (target domain). The presented solution proposes an embeddings extractor based on a Vision Transformer (ViT) and makes use of the k-Nearest-Neighbor (k-NN) algorithm to obtain the anomaly score.",
  pdf={https://dcase.community/documents/challenge2022/technical_reports/DCASE2022_Almudevar_86_t2.pdf},
  preview = {dcase_task2_2022.png},
}

@inproceedings{almudevar23_interspeech,
  bibtex_show={true},
  author={Antonio Almudévar and Alfonso Ortega and Luis Vicente and Antonio Miguel and Eduardo Lleida},
  title={Variational Classifier for Unsupervised Anomalous Sound Detection under Domain Generalization},
  year=2023,
  booktitle={Interspeech},
  abstract={Unsupervised anomalous sound detection typically involves using a classifier with the last layer removed to extract embeddings. After that the cosine distance between train and test embeddings as anomaly score is used. In this paper, we propose a new idea which we call variational classifier that force the embeddings to follow a distribution imposed by design that can depend on the class of the input among other factors. To achieve this goal, in addition to the cross-entropy, we add to the loss function the KL divergence between these distributions and the one followed by the training embeddings. This enhances the ability of the system to differentiate between classes and it allows us to use sampling methods and to calculate the log-likelihood of a test embedding in the train embeddings distributions. We tested this proposal on the DCASE 2022 Task 2 dataset and observed improvements in both classification and unsupervised anomaly detection, which is the primary task.},
  pdf={https://www.isca-archive.org/interspeech_2023/almudevar23_interspeech.pdf},
  preview={variational_class.png},
}

@inproceedings{mariotte2024explainable,
  bibtex_show={true},
  title={An Explainable Proxy Model for Multilabel Audio Segmentation},
  author={Mariotte, Th{\'e}o and Almud{\'e}var, Antonio and Tahon, Marie and Ortega, Alfonso},
  booktitle={ICASSP},
  year={2024},
  abstract={Audio signal segmentation is a key task for automatic audio indexing. It consists of detecting the boundaries of class-homogeneous segments in the signal. In many applications, explainable AI is a vital process for transparency of decision-making with machine learning. In this paper, we propose an explainable multilabel segmentation model that solves speech activity (SAD), music (MD), noise (ND), and overlapped speech detection (OSD) simultaneously. This proxy uses the non-negative matrix factorization (NMF) to map the embedding used for the segmentation to the frequency domain. Experiments conducted on two datasets show similar performances as the pre-trained black box model while showing strong explainability features. Specifically, the frequency bins used for the decision can be easily identified at both the segment level (local explanations) and global level (class prototypes).},
  pdf={https://arxiv.org/pdf/2401.08268},
}

@inproceedings{almudevar2024unsupervised,
  bibtex_show={true},
  title={Unsupervised multiple domain translation through controlled Disentanglement in variational autoencoder},
  author={Almud{\'e}var, Antonio and Mariotte, Th{\'e}o and Ortega, Alfonso and Tahon, Marie},
  booktitle={ICASSP},
  year={2024},
  abstract={Unsupervised Multiple Domain Translation is the task of transforming data from one domain to other domains without having paired data to train the systems. Typically, methods based on Generative Adversarial Networks (GANs) are used to address this task. However, our proposal exclusively relies on a modified version of a Variational Autoencoder. This modification consists on the use of two latent variables disentangled in a controlled way by design. One of this latent variables is imposed to depend exclusively on the domain, while the other one must depend on the rest of the variability factors of the data. Additionally, the conditions imposed over the domain latent variable allow for better control and understanding of the latent space. We empirically demonstrate that our approach works on different vision datasets improving the performance of other well known methods. Finally, we prove that, indeed, one of the latent variables stores all the information related to the domain and the other one hardly contains any domain information.},
  pdf={https://arxiv.org/pdf/2401.09180},
  preview={gm_icassp2024.png},
}

@inproceedings{lebourdais24_interspeech,
  bibtex_show={true},
  title     = {Explainable by-design Audio Segmentation through Non-Negative Matrix Factorization and Probing},
  author    = {Martin Lebourdais and Théo Mariotte and Antonio Almudévar and Marie Tahon and Alfonso Ortega},
  year      = {2024},
  booktitle = {Interspeech},
  abstract={Audio segmentation is a key task for many speech technologies, most of which are based on neural networks, usually considered as black boxes, with high-level performances. However, in many domains, among which health or forensics, there is not only a need for good performance but also for explanations about the output decision. Explanations derived directly from latent representations need to satisfy “good” properties such as informativeness, compactness, or modularity, to be interpretable. In this article, we propose an explainable-by-design audio segmentation model based on non-negative matrix factorization (NMF) which is a good candidate for the design of interpretable representations. This paper shows that our model reaches good segmentation performances, and presents deep analyses of the latent representation extracted from the nonnegative matrix. The proposed approach opens new perspectives toward the evaluation of interpretable representations according to “good” properties.},
  pdf={https://arxiv.org/pdf/2406.13385}
}

@inproceedings{almudevar24_interspeech,
  bibtex_show={true},
  title     = {Predefined Prototypes for Intra-Class Separation and Disentanglement},
  author    = {Antonio Almudévar and Théo Mariotte and Alfonso Ortega and Marie Tahon},
  year      = {2024},
  booktitle = {Interspeech},
  abstract={Prototypical Learning is based on the idea that there is a point (which we call prototype) around which the embeddings of a class are clustered. It has shown promising results in scenarios with little labeled data or to design explainable models. Typically, prototypes are either defined as the average of the embeddings of a class or are designed to be trainable. In this work, we propose to predefine prototypes following human-specified criteria, which simplify the training pipeline and brings different advantages. Specifically, in this work we explore two of these advantages: increasing the inter-class separability of embeddings and disentangling embeddings with respect to different variance factors, which can translate into the possibility of having explainable predictions. Finally, we propose different experiments that help to understand our proposal and demonstrate empirically the mentioned advantages.},
  pdf={https://arxiv.org/pdf/2406.16145}
}

@inproceedings{almudevar2024angular,
  bibtex_show={true},
  title={Angular Distance Distribution Loss for Audio Classification},
  author={Almudévar, Antonio and Serizel, Romain and Ortega, Alfonso},
  booktitle={DCASE Workshop},
  year={2024},
  abstract={Classification is a pivotal task in deep learning not only because of its intrinsic importance, but also for providing embeddings with desirable properties in other tasks. To optimize these properties, a wide variety of loss functions have been proposed that attempt to minimize the intra-class distance and maximize the inter-class distance in the embeddings space. In this paper we argue that, in addition to these two, eliminating hierarchies within and among classes are two other desirable properties for classification embeddings. Furthermore, we propose the Angular Distance Distribution (ADD) Loss, which aims to enhance the four previous properties jointly. For this purpose, it imposes conditions on the first and second order statistical moments of the angular distance between embeddings. Finally, we perform experiments showing that our loss function improves all four properties and, consequently, performs better than other loss functions in audio classification tasks.},
  pdf={https://arxiv.org/pdf/2411.00153}
}

@inproceedings{almudevar2025aligning,
  bibtex_show={true},
  title={Aligning Multimodal Representations through an Information Bottleneck},
  author={Almudévar, Antonio and Hernández-Lobato, José Miguel and Khurana, Sameer and Marxer, Ricard and Ortega, Alfonso},
  booktitle={ICML},
  year={2025},
  abstract={Contrastive losses have been extensively used as a tool for multimodal representation learning. However, it has been empirically observed that their use is not effective to learn an aligned representation space. In this paper, we argue that this phenomenon is caused by the presence of modality-specific information in the representation space. Although some of the most widely used contrastive losses maximize the mutual information between representations of both modalities, they are not designed to remove the modality-specific information. We give a theoretical description of this problem through the lens of the Information Bottleneck Principle. We also empirically analyze how different hyperparameters affect the emergence of this phenomenon in a controlled experimental setup. Finally, we propose a regularization term in the loss function that is derived by means of a variational approximation and aims to increase the representational alignment. We analyze in a set of controlled experiments and real-world applications the advantages of including this regularization term.}, 
  pdf={https://arxiv.org/pdf/2506.04870}
}

@article{almudevar2026rethinking,
  bibtex_show={true},
  title={Rethinking Disentanglement for non-Independent Factors of Variation},
  author={Almud{\'e}var, Antonio and Ortega, Alfonso},
  journal={TMLR},
  year={2026},
  abstract={Representation learning enables the discovery and extraction of underlying factors of variation from data. A representation is typically considered disentangled when it isolates these factors in a way that is interpretable to humans. Existing definitions and metrics for disentanglement often assume that the factors of variation are statistically independent. However, this assumption rarely holds in real-world settings, limiting the applicability of such definitions and metrics in real-world applications. In this work, we propose a novel definition of disentanglement grounded in information theory, which remains valid even when the factors are dependent. We show that this definition is equivalent to requiring the representation to consist of minimal and sufficient variables. Based on this formulation, we introduce a method to quantify the degree of disentanglement that remains effective in the presence of statistical dependencies among factors. Through a series of experiments, we demonstrate that our method reliably measures disentanglement in both independent and dependent settings, where existing approaches fail under the latter.},
  pdf={https://arxiv.org/pdf/2408.07016}
}

@inproceedings{almudevar2026there,
  bibtex_show={true},
  title={There Was Never a Bottleneck in Concept Bottleneck Models},
  author={Almud{\'e}var, Antonio and Hern{\'a}ndez-Lobato, Jos{\'e} Miguel and Ortega, Alfonso},
  booktitle={ICLR},
  year={2026},
  abstract={Deep learning representations are often difficult to interpret, which can hinder their deployment in sensitive applications. Concept Bottleneck Models (CBMs) have emerged as a promising approach to mitigate this issue by learning representations that support target task performance while ensuring that each component predicts a concrete concept from a predefined set. In this work, we argue that CBMs do not impose a true bottleneck: the fact that a component can predict a concept does not guarantee that it encodes only information about that concept. This shortcoming raises concerns regarding interpretability and the validity of intervention procedures. To overcome this limitation, we propose Minimal Concept Bottleneck Models (MCBMs), which incorporate an Information Bottleneck (IB) objective to constrain each representation component to retain only the information relevant to its corresponding concept. This IB is implemented via a variational regularization term added to the training loss. As a result, MCBMs yield more interpretable representations, support principled concept-level interventions, and remain consistent with probability-theoretic foundations.},
  pdf={https://arxiv.org/pdf/2506.04877}
},

@inproceedings{mariotte2026sparse,
  bibtex_show={true},
  title={Sparse Autoencoders Make Audio Foundation Models more Explainable},
  author={Mariotte, Th{\'e}o and Lebourdais, Martin and Almud{\'e}var, Antonio and Tahon, Marie and Ortega, Alfonso and Dugu{\'e}, Nicolas},
  booktitle={ICASSP},
  year={2026},
  abstract={Audio pretrained models are widely employed to solve various tasks in speech processing, sound event detection, or music information retrieval. However, the representations learned by these models are unclear, and their analysis mainly restricts to linear probing of the hidden representations. In this work, we explore the use of Sparse Autoencoders (SAEs) to analyze the hidden representations of pretrained models, focusing on a case study in singing technique classification. We first demonstrate that SAEs retain both information about the original representations and class labels, enabling their internal structure to provide insights into self-supervised learning systems. Furthermore, we show that SAEs enhance the disentanglement of vocal attributes, establishing them as an effective tool for identifying the underlying factors encoded in the representations.},
  pdf={https://arxiv.org/pdf/2509.24793}
},

@article{almudevar2026representation,
  bibtex_show={true},
  title={Representation Unlearning: Forgetting through Information Compression},
  author={Almud{\'e}var, Antonio and Ortega, Alfonso},
  journal={arXiv preprint arXiv:2601.21564},
  year={2026},
  abstract={We present a unified framework for quantifying the similarity between representations through the lens of \textit{usable} information, offering a rigorous theoretical and empirical synthesis across three key dimensions. First, addressing functional similarity, we establish a formal link between stitching performance and conditional mutual information. We further reveal that stitching is inherently asymmetric, demonstrating that robust functional comparison necessitates a bidirectional analysis rather than a unidirectional mapping. Second, concerning representational similarity, we prove that reconstruction-based metrics and standard tools (e.g., CKA, RSA) act as estimators of usable information under specific constraints. Crucially, we show that similarity is relative to the capacity of the predictive family: representations that appear distinct to a rigid observer may be identical to a more expressive one. Third, we demonstrate that representational similarity is sufficient but not necessary for functional similarity. We unify these concepts through a task-granularity hierarchy: similarity on a complex task guarantees similarity on any coarser derivative, establishing representational similarity as the limit of maximum granularity: input reconstruction.},
  pdf={https://arxiv.org/pdf/2601.21568}
}

@article{almudevar2026bridging,
  bibtex_show={true},
  title={Bridging Functional and Representational Similarity via Usable Information},
  author={Almud{\'e}var, Antonio and Ortega, Alfonso},
  journal={Preprint. Under Review},
  year={2026},
  abstract={Machine unlearning seeks to remove the influence of specific training data from a model, a need driven by privacy regulations and robustness concerns. Existing approaches typically modify model parameters, but such updates can be unstable, computationally costly, and limited by local approximations. We introduce Representation Unlearning, a framework that performs unlearning directly in the model's representation space. Instead of modifying model parameters, we learn a transformation over representations that imposes an information bottleneck: maximizing mutual information with retained data while suppressing information about data to be forgotten. We derive variational surrogates that make this objective tractable and show how they can be instantiated in two practical regimes: when both retain and forget data are available, and in a zero-shot setting where only forget data can be accessed. Experiments across several benchmarks demonstrate that Representation Unlearning achieves more reliable forgetting, better utility retention, and greater computational efficiency than parameter-centric baselines.},
  pdf={https://arxiv.org/pdf/2601.21564}
}
