---
---

@article{almudevar2022unsupervised,
  bibtex_show={true},
  title={Unsupervised anomaly detection applied to $\Phi$-OTDR},
  author={Almudévar, Antonio and Sevillano, Pascual and Vicente, Luis and Preciado-Garbayo, Javier and Ortega, Alfonso},
  journal={Sensors},
  volume={22},
  number={17},
  pages={6515},
  year={2022},
  publisher={MDPI},
  abstract={Distributed acoustic sensors (DASs) based on direct-detection Φ-OTDR use the light–matter interaction between light pulses and optical fiber to detect mechanical events in the fiber environment. The signals received in Φ-OTDR come from the coherent interference of the portion of the fiber illuminated by the light pulse. Its high sensitivity to minute phase changes in the fiber results in a severe reduction in the signal to noise ratio in the intensity trace that demands processing techniques be able to isolate events. For this purpose, this paper proposes a method based on Unsupervised Anomaly Detection techniques which make use of concepts from the field of deep learning and allow the removal of much of the noise from the Φ-OTDR signals. The fact that this method is unsupervised means that no human-labeled data are needed for training and only event-free data are used for this purpose. Moreover, this method has been implemented and its performance has been tested with real data showing promising results.},
  preview={sensors-22-06515-g006.png},
  html={https://www.mdpi.com/1424-8220/22/17/6515},
  pdf={https://www.mdpi.com/1424-8220/22/17/6515/pdf},
}

@techreport{AlmudevarUZ2022,
    bibtex_show={true},
    Author = "Almudévar, Antonio and Ortega, Alfonso and Vicente, Luis and Miguel, Antonio and Lleida, Eduardo",
    title = "Vision Transformer based embeddings extractor for Unsupervised Anomalous Sound Detection under Domain Generalization",
    institution = "DCASE2022 Challenge",
    year = "2022",
    abstract = "Anomalous sound detection (ASD) is the task of identifying if a sound is normal or anomalous with respect to a given reference. In most scenarios, we have a large amount of normal data to design our model, but little or no anomalous data. When this situation occurs, the problem can be approached in an unsupervised manner, i.e., only normal data is used for design. In this report we present a solution for the DCASE2022 task 2 (Unsupervised Anomalous Sound Detection for Machine Condition Monitoring Applying Domain Generalization Techniques), which aims to address the ASD problem under domain generalization. This means that the data to develop the system belongs to the source domain, while the test data can belong to this domain or to a different one (target domain). The presented solution proposes an embeddings extractor based on a Vision Transformer (ViT) and makes use of the k-Nearest-Neighbor (k-NN) algorithm to obtain the anomaly score.",
    preview = "dcase_task2_2022.png",
    html={https://dcase.community/challenge2022/task-unsupervised-anomalous-sound-detection-for-machine-condition-monitoring-results#AlmudevarUZ2022},
    pdf={https://dcase.community/documents/challenge2022/technical_reports/DCASE2022_Almudevar_86_t2.pdf},
}

@inproceedings{almudevar23_interspeech,
  bibtex_show={true},
  author={Antonio Almudévar and Alfonso Ortega and Luis Vicente and Antonio Miguel and Eduardo Lleida},
  title={{Variational Classifier for Unsupervised Anomalous Sound Detection under Domain Generalization}},
  year=2023,
  booktitle={Proc. INTERSPEECH 2023},
  pages={2823--2827},
  abstract={Unsupervised anomalous sound detection typically involves using a classifier with the last layer removed to extract embeddings. After that the cosine distance between train and test embeddings as anomaly score is used. In this paper, we propose a new idea which we call variational classifier that force the embeddings to follow a distribution imposed by design that can depend on the class of the input among other factors. To achieve this goal, in addition to the cross-entropy, we add to the loss function the KL divergence between these distributions and the one followed by the training embeddings. This enhances the ability of the system to differentiate between classes and it allows us to use sampling methods and to calculate the log-likelihood of a test embedding in the train embeddings distributions. We tested this proposal on the DCASE 2022 Task 2 dataset and observed improvements in both classification and unsupervised anomaly detection, which is the primary task.},
  doi={10.21437/Interspeech.2023-1965},
  preview={variational_class.png},
  html={https://www.isca-speech.org/archive/interspeech_2023/almudevar23_interspeech.html},
  pdf={https://www.isca-speech.org/archive/pdfs/interspeech_2023/almudevar23_interspeech.pdf},
}

@inproceedings{almudevar2024variational,
  bibtex_show={true},
  title={Unsupervised Multiple Domain Translation through Controlled Disentanglement in Variational Autoencoder},
  author={Almudévar, Antonio and Mariotte, Théo and Ortega, Alfonso and Tahon, Marie},
  booktitle={ICASSP 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year={2024},
  organization={IEEE},
  abstract={Unsupervised Multiple Domain Translation is the task of transforming data from one domain to other domains without having paired data to train the systems. Typically, methods based on Generative Adversarial Networks (GANs) are used to address this task. However, our proposal exclusively relies on a modified version of a Variational Autoencoder. This modification consists on the use of two latent variables disentangled in a controlled way by design. One of this latent variables is imposed to depend exclusively on the domain, while the other one must depend on the rest of the variability factors of the data. Additionally, the conditions imposed over the domain latent variable allow for better control and understanding of the latent space. We empirically demonstrate that our approach works on different vision datasets improving the performance of other well known methods. Finally, we prove that, indeed, one of the latent variables stores all the information related to the domain and the other one hardly contains any domain information.},
  preview={gm_icassp2024.png},
  pdf={ICASSP_2024_var_translation.pdf}
}

@inproceedings{mariotte2024nmf,
  bibtex_show={true},
  title={An Explainable Proxy Model for Multilabel Audio Segmentation},
  author={Mariotte, Théo and Almudévar, Antonio and Tahon, Marie and Ortega, Alfonso},
  booktitle={ICASSP 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year={2024},
  organization={IEEE},
  abstract={Audio signal segmentation is a key task for automatic audio indexing. It consists of detecting the boundaries of class-homogeneous segments in the signal. In many applications, explainable AI is a vital process for transparency of decision-making with machine learning. In this paper, we propose an explainable multilabel segmentation model that solves speech activity (SAD), music (MD), noise (ND), and overlapped speech detection (OSD) simultaneously. This proxy uses the non-negative matrix factorization (NMF) to map the embedding used for the segmentation to the frequency domain. Experiments conducted on two datasets show similar performances as the pre-trained black box model while showing strong explainability features. Specifically, the frequency bins used for the decision can be easily identified at both the segment level (local explanations) and global level (class prototypes).}
}
